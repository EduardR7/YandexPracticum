
# YandexPracticum
Проекты, выполненные на курсе Data Science на Яндекс.Практикум.


# Список проектов:
### 09. [*Технологический процесс*] (https://github.com/EduardR7/YandexPracticum/blob/09_tech_process/09_tech_process_git.ipynb)
**Описание проекта:**
1. Необходимо оптимизировать технологический процесс получения золота из руды.
2. Оптимизация техпроцесса подразумевает оптимизацию recovery: recovery_pred = ((C * (F - T)) / (F * (C - T))) * 100
3. В качестве метрики качества использовать — sMAPE (англ. Symmetric Mean Absolute Percentage Error, «симметричное среднее абсолютное процентное отклонение»).

**Результат:**
1. Путем исследования трех моделей регрессии, удалось выявить наилучшую, с наилучшими показателями sMAPE, этой моделью оказался алгоритм DummyRegressor.
2. Худшие показатели были замечены у модели случайного леса. Вероятно такие результаты связаны с несбалансированностью target.
3. В ходе подсчета итогового sMAPE мы получили результат 8.03%.

**Инструменты и техники:**
Python, matplotlib, sklearn, seaborn

**Статус проекта:**
Закончен


### 10. [*Защита персональных данных клиентов*] (https://github.com/EduardR7/YandexPracticum/blob/toxic_coments/toxic1.ipynb)
**Описание проекта:**
1. Необходимо защитить данные клиентов страховой компании «Хоть потоп». Разработайте такой метод преобразования данных, чтобы по ним было сложно восстановить персональную информацию. Обоснуйте корректность его работы.
2. Нужно защитить данные, чтобы при преобразовании качество моделей машинного обучения не ухудшилось. Подбирать наилучшую модель не требуется.

**Результат:**
1. Для решения задачи сделаны предварительные теоретические выводы. Указано, что при умножении матрицы с признаками на случайную матрицу, качество оценки не должно уменьшиться.
2. Исходный набор разделен на матрицу признаков (X) и вектор таргетов, y.
3. Матрица признаков умножена на обратимую слуайную матрицу (4х4), получена матрица X1.
4. Обе матрицы и таргеты разбиты на наборы train и test. На train обучены две модели линейной регресси. С помощью моделей сделаны два предсказания на тестовых наборах (исходном X_test) и преобразованном (X1_test).
5. Оба набора сгенерировали практически идентичные наборы y_test_pred y1_test_pred. Метрики R2 совпали до 11 знака после запятой.
6. Численный расчет подтверждает теоретические выводы.

**Инструменты и техники:**
Numpay, Pandas, sklearn, seaborn

**Статус проекта:**
Закончен

### 13. [*Проект для «Викишоп», с использованием BERT*] (https://github.com/EduardR7/YandexPracticum/blob/toxic_coments/toxic1.ipynb)
**Описание проекта:**
1. Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию.
2. Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.
3. Решить задачу можно как с помощью BERT, так и без этой нейронки. Если хотите попробовать BERT — Выполните проект локально. Упомяните BERT в заголовке проекта в первой ячейке.

**Результат:**
1. Задача оценки токсичных коментариев сделана с использованием предтестированных моделей BERT.
2. Данные предварительно очищены. Выявлен дисбаланс по таргетам.
3. Токенизация, эмбединг и padding выполнены с помощью BERT.
4. Самая длительная процедура - оптимизация векторов признаков, после padding. После оптимизации, получилось всего 6 признаков. 
5. Было выполнено разделение target и признаки, train и test.
6. Затем создается цикл поиска оптимальных гиперпараметров. В каждом цикле производится разделение выборки train на – valid (1/5) и train (4/5) объема. Для Upsampling применяется SMOT. Применение стандартных методик кросс валидации приводит к ошибочному завышению f1 на 12%.
7. Для предсказаний использовалась линейная регрессия. Результат хороший:
- f1 на валидации 85%,
- f1 на test 83%.

**Инструменты и техники:**
Python, лемматизация, предобработка данных,
torch, transformers, tqdm, pandas, numpy, sklearn, imblearn

**Статус проекта:**
Закончен
